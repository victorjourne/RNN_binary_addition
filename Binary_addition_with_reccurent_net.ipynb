{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand Recurrent Neural Network with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neural network (RNN) is a neural network with a feedback loop. Contrary to a standard neural network, a sample got an extra dimension, a sequencing. In Keras, it is called the timestep dimension.\n",
    "\n",
    "\n",
    "This kind of network aims to keep in mind its states accross the timesteps.  \n",
    "The fundamental equation of a simple RNN, expressed at the current timestep $t$ is:\n",
    "\n",
    "$$o^t = f(h^t; \\theta) $$\n",
    "\n",
    "$$h^t = g(h^{t-1}, x^t; \\theta)$$\n",
    "\n",
    "Where $o$ is the output, $h$ the hidden state and $x$ the input. $\\theta$ are the weights to be adjusted of the RNN.\n",
    "$f,g$ are activation functions, as sigmoid, Relu...\n",
    "![title](RNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This notebook outlines the Keras implementation for a very simple RNN, which is able to add octets (8 bits binary numbers).\n",
    "This example is perfect to understand what a reccurent networks are designed for; keep hidden state in memory. Here, we want the RNN to be able to add the carry bit.\n",
    "\n",
    "I have been inspired by the great explanation found [here](http://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/). This page gives RNN code step by step, giving good insight of how the backpropagation works for this kind of network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, a sample $x$ is made of two octets and the output $o$ is the addition result one.\n",
    "For instance; $x =[[00110010],[00010010]], o=[01000100]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us generate a binary mapper and a integer mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = np.random.seed(0)\n",
    "# int2binary\n",
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "# binary2int\n",
    "def binary2int(b):\n",
    "    out = 0\n",
    "    for index,x in enumerate(b):\n",
    "            out += x*pow(2,index)\n",
    "#            print(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now convert easily any integer smaller than 256 to binary representation. For our addition problem, we limit the inputs number to being  smaller than 128 in order to keep the output smaller than 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [[0 0 1 0 1 1 0 0]\n",
      " [0 0 1 0 1 1 1 1]] \n",
      " output [0 1 0 1 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# example of binary mapper\n",
    "a_int,b_int =  np.random.randint(largest_number/2,size = 2) \n",
    "c_int = a_int + b_int\n",
    "x = np.stack([int2binary[a_int],int2binary[b_int]])\n",
    "o = int2binary[c_int]\n",
    "print('inputs %s \\n output %s'%(x,o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to keep in mind to reverse the inputs so as to  feed the RNN with first small bit coding $2^0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input variables\n",
    "alpha = 0.1\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "nb_ex = 10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rnn (SimpleRNN)              (None, 8, 16)             288       \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 8, 1)              16        \n",
      "=================================================================\n",
      "Total params: 304.0\n",
      "Trainable params: 304\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(hidden_dim, name='rnn',\n",
    "                    input_shape=(binary_dim,2),\n",
    "                    stateful=False,\n",
    "                    activation='sigmoid',\n",
    "                    return_sequences=True,\n",
    "                    use_bias =False))\n",
    "model.add(TimeDistributed(Dense(units = 1,name='output',\n",
    "                                activation  =\"sigmoid\",\n",
    "                                use_bias=False)))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How wen can see, we have 304 parameters to train. In fact, there is:\n",
    "\n",
    "$\\theta_i$ has 2x16 weights\n",
    "\n",
    "$\\theta_h$ has 16x16 weights\n",
    "\n",
    "$\\theta_o$ has 16x1 weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep close to [this implementation](http://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/) again, we look at the **sum of the absolute error** of the output neuron over the 8 timestamps.\n",
    "\n",
    "Insteed of using the mean square error loss:\n",
    "$$ C = \\frac{(y-a)^2}{2}$$\n",
    "\n",
    "we can use the binary cross entropy of Keras.\n",
    "$$C = - y \\ln a + (1-y ) \\ln (1-a) $$\n",
    "\n",
    "The cost function computed at each timestamp is now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom metrics to show\n",
    "def overall_error(y_true,y_pred):\n",
    "    return K.sum(K.abs(y_true-y_pred))\n",
    "# stochastic gradient descent strategy\n",
    "sgd = SGD(lr=alpha, momentum=0.0, decay=0.0, nesterov=False)\n",
    "# loss: binar\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, \n",
    "              metrics=[overall_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34+54?=4.0\n",
      "0.641955435276\n",
      "78+109?=187.0\n",
      "0.0652224719524\n",
      "51+94?=145.0\n",
      "0.020537737757\n",
      "47+48?=95.0\n",
      "0.00928809680045\n",
      "35+27?=62.0\n",
      "0.00299859023653\n",
      "101+100?=201.0\n",
      "0.0025868860539\n",
      "99+37?=136.0\n",
      "0.0027393035125\n",
      "65+106?=171.0\n",
      "0.00234018452466\n",
      "73+108?=181.0\n",
      "0.0016108098207\n",
      "114+71?=185.0\n",
      "0.00154325680342\n"
     ]
    }
   ],
   "source": [
    "for ex in range(nb_ex):\n",
    "    # data shape = (steps, inputs/outputs)\n",
    "    a_int,b_int =  np.random.randint(largest_number/2,size = 2) \n",
    "    c_int = a_int + b_int\n",
    "    X_ex = np.stack([int2binary[a_int],int2binary[b_int]]).T\n",
    "    # reverse the inputs; to feed the RNN with first bits\n",
    "    X_ex = np.reshape(X_ex[::-1],(1,binary_dim,2))\n",
    "    Y_ex = int2binary[c_int]\n",
    "    Y_ex = np.reshape(Y_ex[::-1],(1,binary_dim,1))\n",
    "    #a gradient descent at each example\n",
    "    loss,metrics = model.train_on_batch(X_ex, Y_ex)\n",
    "    ex += 1    \n",
    "    if ex % 10**4 == 0:\n",
    "        pred = model.predict_on_batch(X_ex).round()\n",
    "        pred_int = binary2int(np.reshape(pred,binary_dim))\n",
    "        print('%s+%s?=%s'%(a_int,b_int,pred_int))\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 200 000 examples, the RNN knows how to add the carry bit. To be convinced of this, let's try with \"the most\" difficult example, where the carry bit crosses the whole octet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      " [[1 1 1 1 1 1 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "output: \n",
      " [[ 1.  1.  1.  1.  1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "a_int = largest_number-2\n",
    "b_int=1\n",
    "X_ex = np.stack([int2binary[a_int],int2binary[b_int]]).T\n",
    "print(\"input: \\n %s\"%X_ex.T)\n",
    "X_ex = np.reshape(X_ex[::-1],(1,binary_dim,2))\n",
    "pred = model.predict_on_batch(X_ex).round()\n",
    "print(\"output: \\n %s\"%pred[0].T[::-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Some remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "The gradient is updated between two samples. A sample is presented to the network, the feedforward pass computes the 8 bits output, and then the gradient is backpropagated from the output layer at the last timestep (encoding $2^8$) to the input layer of the first timestep (endcding $2^0$). Exactly as if it was a simple neural network 8 times unrolled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *TimeDistributed()* and *return_sequences=True*  effects\n",
    "We have used  *model.add(TimeDistributed(Dense(...* layer in combination with *return_sequences=True*, rather than *model.add(Dense(...* layer, in order to produce an output with the same sequence's lenth than the inputs'one. Indeed, $o$ is a 8 bits long. This architecture is called in the litterature *many-to-many* predictions.\n",
    "\n",
    "*return_sequences=True* in combination with *model.add(Dense(...* would have produced a 8bits long output, but the hidden layer would have been fully connected with the output layer across the timesteps, leading to more parameters to train, and differant predictions. *model.add(TimeDistributed(Dense(...* enables to keep the same hidden layer during over the timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "[This implementation](http://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/) uses squared loss to compute the gradient, in this way : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might use rather:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_2_deltas.append(layer_2_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a cross-entropy loss, more natural here."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
